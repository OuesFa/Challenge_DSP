{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. La démarche:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons souhaité réaliser une étude qui suit le cheminement d'une démarche d'étude  : à comprendre acquérir la donnée, l'observer, se l'approprier et la nettoyer. Suite à cela nous appliquons nos algorithmes de machine learning afin de prédire notre cible. Néanmoins le chemin est plus ardu qu'il n'y parait : il est nécessaire pour chaque model de paramétrer l'algorithme choisi afin d'en tirer les meilleurs performances, mais très vite on s'aperçoit que les résultats atteignent rapidement un plafond que l'on ne peut améliorer seulement en retravaillant le tuning de l'algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gain le plus important sur les performances est atteint en retraitant la donnée la plus intelligemment possible, dans notre situation, avec des retraitements simples on obtient des gains de performances notables tant bien pour la métrique RMSE de prédilection que pour d'autres métriques (tel que MAPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data management:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Régression sur les valeurs manquantes\n",
    "- Imputation des données catégorielles par leur top représentant\n",
    "- Encodage disjonctif complet (dummy) sur les variables catégorielles\n",
    "- Traitement des outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Choix des algorithmes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1er. Régression Linéaire Lasso sur données polynomiales d'ordre 2\n",
    "- 2nd. Régression Linéaire standard (sans pénalisation) sur données polynomiales d'ordre 2\n",
    "- 3eme. XGBoost (profondeur 4, nb_estimateur : 400, learning rate à 0.05) sur donnée d'ordre 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>IMPORTANT : </font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les notebooks fournis font office de rapport et de code à la fois, il est possible de les exécuter néanmoins les affichages seront réinitialisés et il sera nécessaire d'attendre la fin de l'exécution du notebook.\n",
    "- L'ordre d'exécution des notebooks est important, il est nécessaire de les exécuter dans l'ordre suivant :\n",
    "    - Ch1 -Data Exploration _ Préparation.ipynb\n",
    "    - Ch2 -Apprentissage.ipynb\n",
    "    - Ch3 -Fine Tuning & Améliorations.ipynb\n",
    "- Un script d'exécution pure est également fourni pour régénérer le fichier, néanmoins pour les explications il est nécessaire de se référer aux notebooks ci-dessus.\n",
    "- Il est possible de consulter les notebooks en ligne (affiche légèrement altérée) directement sur mon repo github au lien suivant : https://github.com/dvp-tran/Axa_Challenge\n",
    "    Il est possible de cloner le repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
